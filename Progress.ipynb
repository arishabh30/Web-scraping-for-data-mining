{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "from random import randint\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettingUrl(url):\n",
    "\n",
    "    options = Options()\n",
    "    ua = UserAgent()\n",
    "    userAgent = ua.random\n",
    "    # print(userAgent)\n",
    "    options.headless = True  # setting this to true makes the browser work in the background\n",
    "    options.add_argument(\"--proxy-server='direct://'\")\n",
    "    options.add_argument(\"--proxy-bypass-list=*\")\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument('--lang=en_US')\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--ignore-certificate-errors\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(f'user-agent={userAgent}')\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    # DRIVER_PATH = \"/usr/bin/chromedriver\"\n",
    "    driver = webdriver.Chrome(service=Service(\n",
    "        ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    driver.get_screenshot_as_file(\"screenshot1.png\")\n",
    "    time.sleep(randint(10,30))\n",
    "\n",
    "    html = driver.page_source\n",
    "    # this renders the JS code and stores all of this information in static HTMl code.\n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting data from acs nano site\n",
    "html = gettingUrl(\"https://pubs.acs.org/doi/10.1021/acsnano.5b05040\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding out the number of times the research paper number 5 is mentioned in the text.\n",
    "# creating an instance of the BeautifulSoup library\n",
    "soup_new = BeautifulSoup(html, 'lxml')\n",
    "a_tags = soup_new.find('div', class_=\"article_content-left ui-resizable\")\n",
    "# print(a_tags)\n",
    "count = a_tags.find_all('a', class_=\"ref5\")\n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to find the last author names of the references\n",
    "soup_new = BeautifulSoup(html, 'lxml')\n",
    "allLastAuthors =[]\n",
    "\n",
    "relevant = soup_new.find('div', class_=\"article_content-left ui-resizable\")\n",
    "string_Refs = relevant.find('p', class_=\"references-count\")\n",
    "string = string_Refs.text\n",
    "number_Refs = re.findall(r'\\d+', string)\n",
    "\n",
    "print(number_Refs[0])\n",
    "\n",
    "i=0\n",
    "\n",
    "for i in range(1, int(number_Refs[0])+1):\n",
    "\n",
    "    info = relevant.find('li', {\"id\":\"ref\"+str(i)})\n",
    "\n",
    "    if info is None:\n",
    "        allLastAuthors.append(\"No information\")\n",
    "    # print(info)\n",
    "\n",
    "    authors = info.find_all('span',class_='NLM_contrib-group')\n",
    "    # print(authors)\n",
    "    last_author = authors[-1].text\n",
    "    allLastAuthors.append(last_author)\n",
    "    i+=1\n",
    "\n",
    "# print((allLastAuthors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an instance of the BeautifulSoup library\n",
    "soup2 = BeautifulSoup(html, 'lxml')\n",
    "links = soup2.find('div', class_=\"article_content-left ui-resizable\")\n",
    "# creating a list containing the google scholar links for all the reference papers.\n",
    "paperLinksgoogleScholar = links.find_all('a', class_=\"google-scholar\")\n",
    "scholarLinks = []\n",
    "for link in paperLinksgoogleScholar:\n",
    "    # A new list containing the google scholar links for only the first three papers is created.\n",
    "    scholarLinks.append(link['href'])\n",
    "for link in scholarLinks:\n",
    "    index = scholarLinks.index(link)\n",
    "    print(index+1, link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_new = []\n",
    "for link in scholarLinks:\n",
    "    html = gettingUrl(link)\n",
    "    # creating an instance of the BeautifulSoup library\n",
    "    soup3 = BeautifulSoup(html, 'lxml')\n",
    "    content = soup3.find('div', class_=\"gs_a\")\n",
    "\n",
    "    if content == None:\n",
    "        print(\"No Data Found\")\n",
    "    else:\n",
    "        authors = content.find_all('a')\n",
    "\n",
    "        # getting the name of the professor to then set up further relationships.\n",
    "        print(authors[-1].text)\n",
    "        authors_new.append(authors[-1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the publication house name of the referenced paper from google scholar.\n",
    "\n",
    "name = content.text\n",
    "# print(name)\n",
    "\n",
    "if \"science\" in name:\n",
    "    print(\"science\")\n",
    "elif 'Wiley' in name:\n",
    "    print(\"wiley\")\n",
    "elif 'nature' in name:\n",
    "    print(\"nature\")\n",
    "elif 'sciencedirect' in name:\n",
    "    print(\"sciencedirect\")\n",
    "elif 'Springer' in name:\n",
    "    print(\"Springer\")\n",
    "elif 'Elsevier' in name:\n",
    "    print(\"Elsevier\")\n",
    "elif 'mdpi' in name:\n",
    "    print(\"mdpi\")\n",
    "elif 'ACS Publications' in name:\n",
    "    print(\"ACS Publications\")\n",
    "elif 'ieee' in name:\n",
    "    print(\"ieee\")\n",
    "else:\n",
    "    print(\"other\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding out the publication house link of the paper from google scholar.\n",
    "paperLink = soup3.find('h3', class_='gs_rt')\n",
    "print(paperLink.a['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding out the name of the research paper by extracting the text from the google scholar page.\n",
    "\n",
    "options = Options()\n",
    "ua = UserAgent\n",
    "userAgent = ua\n",
    "options.headless = True\n",
    "options.add_argument(\"--proxy-server='direct://'\")\n",
    "options.add_argument(\"--proxy-bypass-list=*\")\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument('--lang=en_US')\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "options.add_argument(\"--ignore-certificate-errors\")\n",
    "options.add_argument(\"--disable-extensions\")\n",
    "options.add_argument(f'user-agent={userAgent}')\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(\n",
    "    ChromeDriverManager().install()), options=options)\n",
    "driver.get(scholarLinks[4])\n",
    "# driver.find_element(By.XPATH, value='//*[@id=\"gs_res_ccl_mid\"]/div/div[2]/div[3]/a[2]').click()\n",
    "driver.find_element(By.LINK_TEXT, \"Cite\").click()\n",
    "time.sleep(randint(10, 30))\n",
    "\n",
    "# To get the name of the 5th paper\n",
    "html = gettingUrl(driver.current_url)\n",
    "# print(driver.current_url)\n",
    "soup4 = BeautifulSoup(html, 'lxml')\n",
    "paperName = soup4.find('div', class_='gs_citr')\n",
    "print(paperName.i.text)  # printing the name of the 5th paper.\n",
    "\n",
    "# quits the whole browser session along with all the associated browser windows, tabs and pop-ups.\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction from \"Nature\" publication house.\n",
    "html = gettingUrl(\"https://www.nature.com/articles/s41417-022-00495-w\")\n",
    "\n",
    "\n",
    "# to get the google scholar links of the references.\n",
    "paperLinksgoogleScholarnature = []\n",
    "# creating an instance of the BeautifulSoup library\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "links = soup.find('ol', class_=\"c-article-references\")\n",
    "tags = links.find_all('a', {'data-track-action': 'google scholar reference'})\n",
    "for tag in tags:\n",
    "    # creating a list containing the google scholar links for all the\n",
    "    paperLinksgoogleScholarnature.append(tag['href'])\n",
    "\n",
    "\n",
    "# to get the doi of the references.\n",
    "natureDOI = []\n",
    "refNumber = len(paperLinksgoogleScholarnature)\n",
    "\n",
    "i=0\n",
    "\n",
    "for i in range(1, int(refNumber)+1):\n",
    "    doiLinks = links.find('a', {'aria-label': 'Article reference '+str(i)})\n",
    "\n",
    "    if doiLinks == None:\n",
    "        natureDOI.append(\"No DOI\")\n",
    "    else:\n",
    "        natureDOI.append(doiLinks['href'])\n",
    "    i+=1\n",
    "\n",
    "\n",
    "# extracting the authors from the main page itself.\n",
    "text = soup.find_all('ol', class_='c-article-references')\n",
    "# print(text)\n",
    "text_main = soup.find_all(\n",
    "    'li', class_='c-article-references__item js-c-reading-companion-references-item')\n",
    "# print(text_main[0])\n",
    "\n",
    "Titles = []\n",
    "allLastAuthors_nature = []\n",
    "journalName = []\n",
    "yearPublication = []\n",
    "\n",
    "for ref in text_main:\n",
    "    string = ref.find('p', class_='c-article-references__text')\n",
    "\n",
    "    togetdeets = string.text.split('.')\n",
    "\n",
    "    # returns authors in the form of a string.\n",
    "    allAuthors = togetdeets[0]\n",
    "    allAuthorsList = allAuthors.split(',')  # converting to a list\n",
    "\n",
    "    Titles.append(togetdeets[1])  # gets the title of the references\n",
    "    \n",
    "    journalName.append(togetdeets[2])# gets the journal name of the references\n",
    "\n",
    "    if len(togetdeets) < 4:\n",
    "        yearPublication.append(' ') #gets the year of publication of the references\n",
    "    else:\n",
    "        year = togetdeets[3].split(';')\n",
    "        yearPublication.append(year[0])\n",
    "\n",
    "    if allAuthorsList[-1] == ' et al':\n",
    "        allLastAuthors_nature.append(allAuthorsList[-2])\n",
    "    else:\n",
    "        allLastAuthors_nature.append(allAuthorsList[-1])\n",
    "\n",
    "print(yearPublication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction from \"Springer\" publication house.\n",
    "html = gettingUrl(\"https://link.springer.com/article/10.1007/s43673-022-00064-1\")\n",
    "paperLinksgoogleScholarspringer = []\n",
    "# creating an instance of the BeautifulSoup library\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "links = soup.find('ol', class_=\"c-article-references\")\n",
    "refNumber = len(links)\n",
    "k = 0\n",
    "for k in range(1, int(refNumber)+1):\n",
    "    tags = links.find('a', {'aria-label': 'Google Scholar reference ' +str(k)})\n",
    "    if tags==None:\n",
    "        paperLinksgoogleScholarspringer.append(\"No Google Scholar Link\")\n",
    "    else:\n",
    "        paperLinksgoogleScholarspringer.append(tags['href'])\n",
    "    k+=1\n",
    "# print(paperLinksgoogleScholarspringer)\n",
    "\n",
    "\n",
    "\n",
    "#to get the doi of the references.\n",
    "springerDOI = []\n",
    "refNumber = len(links)\n",
    "# print(refNumber)\n",
    "\n",
    "i=0\n",
    "\n",
    "for i in range(1, refNumber+1):\n",
    "    doiLinks = links.find('a', {'aria-label': 'Article reference '+str(i)})\n",
    "    # print(doiLinks)\n",
    "\n",
    "    if doiLinks == None:\n",
    "        springerDOI.append(\"No DOI\")\n",
    "    else:\n",
    "        springerDOI.append(doiLinks['href'])\n",
    "    i+=1\n",
    "\n",
    "# print(springerDOI)\n",
    "\n",
    "\n",
    "#extracting the authors from the main page itself.\n",
    "text = soup.find_all('ol', class_='c-article-references')\n",
    "\n",
    "k = 0\n",
    "\n",
    "\n",
    "text_main = soup.find_all('li', class_='c-article-references__item js-c-reading-companion-references-item')\n",
    "\n",
    "Titles = []\n",
    "allLastAuthors_springer = []\n",
    "journalName = []\n",
    "yearPublication = []\n",
    "\n",
    "for ref in text_main:\n",
    "    string = ref.find('p', class_=\"c-article-references__text\")\n",
    "\n",
    "    togetdeets = string.text.split(',')\n",
    "    content = togetdeets[-2].split('.')\n",
    "    Titles.append(content[0]) # gets the title of the references\n",
    "\n",
    "    if len(togetdeets)<4:\n",
    "        allLastAuthors_springer.append(\"No Data Found\")\n",
    "    else:\n",
    "        allLastAuthors_springer.append(togetdeets[-3])\n",
    "\n",
    "    sum = ''\n",
    "    for j in range(1, len(content)-1):\n",
    "        sum += content[j]\n",
    "    journalName.append(sum) # gets the journal name of the references\n",
    "\n",
    "    year = togetdeets[-1].split('(')\n",
    "    # print(year[-1])\n",
    "\n",
    "    if year[-1][0]==\" \":\n",
    "        yearPublication.append(\"No Data Found\") # gets the year of publication of the references\n",
    "    else:\n",
    "        year_new = year[-1].split(')')\n",
    "        yearPublication.append(year_new[0]) #gets the year of publication of the references\n",
    "\n",
    "\n",
    "# print(len(springerDOI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mint\u001b[39m(refNumber)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     tag \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mdd\u001b[39m\u001b[39m'\u001b[39m, {\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msref\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39;49m(i)})\n\u001b[1;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(tag)\n\u001b[1;32m     22\u001b[0m \u001b[39m# for i in range(1,int(refNumber)+1):\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m#     tag = soup.find('dd', {'id':'sref'+str(i)})\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m#     print(tag)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39m# for tag in tags:\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m#     Titles.append(((tag.find(\"div\", class_=\"contribution\")).find('strong',class_='title')).text)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "# extraction from \"Elsevier(sciencedirect)\" publication house.\n",
    "# html = gettingUrl(\"https://www.sciencedirect.com/science/article/pii/S2376060522000608\")\n",
    "paperLinksgoogleScholarelsevier = []\n",
    "links = []\n",
    "Titles =[]\n",
    "elsevierDOI = []\n",
    "# creating an instance of the BeautifulSoup library\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "tags = soup.find_all('dd', class_=\"reference\")\n",
    "\n",
    "refNumber = len(tags)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for i in range(1, int(refNumber)+1):\n",
    "    tag = soup.find('dd', {'id': 'sref'+str(i)})\n",
    "    print(tag)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# for i in range(1,int(refNumber)+1):\n",
    "#     tag = soup.find('dd', {'id':'sref'+str(i)})\n",
    "#     print(tag)\n",
    "\n",
    "# for tag in tags:\n",
    "#     link = (tag.find_all('a', class_=\"link\"))\n",
    "#     for l in link:\n",
    "#         links.append(l['href'])\n",
    "    \n",
    "#     doi = tag.find('a', class_=\"link\")\n",
    "#     elsevierDOI.append(\"https://www.sciencedirect.com/\"+doi['href'])\n",
    "    \n",
    "\n",
    "# for link in links:\n",
    "#     if \"scholar\" in link:\n",
    "#         paperLinksgoogleScholarelsevier.append(link)\n",
    "\n",
    "# for link in paperLinksgoogleScholarelsevier:\n",
    "#     index = paperLinksgoogleScholarelsevier.index(link)\n",
    "\n",
    "# for tag in tags:\n",
    "#     Titles.append(((tag.find(\"div\", class_=\"contribution\")).find('strong',class_='title')).text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction from \"MDPI\" publication house.\n",
    "paperLinksgoogleScholarMDPI = []\n",
    "html = gettingUrl(\"https://www.mdpi.com/2226-4310/9/11/704/htm\")\n",
    "# creating an instance of the BeautifulSoup library\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "tags = soup.find('ol', class_=\"html-xx\")\n",
    "links = tags.find_all('a', class_=\"google-scholar\")\n",
    "for link in links:\n",
    "    paperLinksgoogleScholarMDPI.append(link['href'])\n",
    "    index = links.index(link)\n",
    "    print(index+1, link['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction from \"Science\" publication house.\n",
    "html = gettingUrl(\"https://science.sciencemag.org/content/371/6534/eaay9982\")\n",
    "paperLinksgoogleScholarScienceall = []\n",
    "paperLinksgoogleScholarScience = []\n",
    "# creating an instance of the BeautifulSoup library\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "tags = soup.find('section', {'id': 'bibliography'})\n",
    "links = tags.find_all('a')\n",
    "for link in links:\n",
    "    paperLinksgoogleScholarScienceall.append(link['href'])\n",
    "\n",
    "for link in paperLinksgoogleScholarScienceall:\n",
    "    if \"scholar\" in link:\n",
    "        paperLinksgoogleScholarScience.append(link)\n",
    "        index = paperLinksgoogleScholarScience.index(link)\n",
    "        print(index+1, link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction from \"IEEE\" publication house.\n",
    "html = gettingUrl(\"https://ieeexplore.ieee.org/document/9837920/references#references\")\n",
    "paperLinksgoogleScholarIEEE = []\n",
    "paperLinksgoogleScholarIEEEfinal = []\n",
    "# creating an instance of the BeautifulSoup library\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "tags = soup.find_all('div', class_='reference-container')\n",
    "for tag in tags:\n",
    "    link = tag.find('a', class_=\"stats-reference-link-googleScholar\")\n",
    "    paperLinksgoogleScholarIEEE.append(link['href'])\n",
    "\n",
    "initial = paperLinksgoogleScholarIEEE[0]\n",
    "paperLinksgoogleScholarIEEEfinal.append(initial)\n",
    "i = 1\n",
    "while (i < len(paperLinksgoogleScholarIEEE)):\n",
    "    if paperLinksgoogleScholarIEEE[i] != initial:\n",
    "        paperLinksgoogleScholarIEEEfinal.append(paperLinksgoogleScholarIEEE[i])\n",
    "    else:\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "for link in paperLinksgoogleScholarIEEEfinal:\n",
    "    index = paperLinksgoogleScholarIEEEfinal.index(link)\n",
    "    print(index+1, link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction from \"Cambridge publishing house\" publication house.\n",
    "html = gettingUrl(\"https://www.cambridge.org/core/journals/experimental-results/article/observing-nonuniform-nonluders-yielding-in-a-coldrolled-medium-manganese-steel-with-digital-image-correlation/1C8B6F3364BA54FC2C1D7801FCFDB85E\")\n",
    "paperlinksCambridge = []\n",
    "paperlinksgoogleScholarCambridge = []\n",
    "\n",
    "# creating an instance of the BeautifulSoup library\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "tags = soup.find('div', {'id': 'references-list'})\n",
    "links = tags.find_all('a', class_=\"ref-link\")\n",
    "for link in links:\n",
    "    paperlinksCambridge.append(link['href'])\n",
    "for link in paperlinksCambridge:\n",
    "    if \"scholar\" in link:\n",
    "        paperlinksgoogleScholarCambridge.append(link)\n",
    "        index = paperlinksgoogleScholarCambridge.index(link)\n",
    "        print(index+1, link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Publication(link):\n",
    "    x = re.findall('\\.(.*?)\\.', link)\n",
    "    return x\n",
    "\n",
    "print(Publication(\"https://link.springer.com/article/10.1007/s43673-022-00064-1\"))\n",
    "print(Publication(\"https://www.nature.com/articles/s41417-022-00495-w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"https://link.springer.com/article/10.1007/s43673-022-00064-1\"\n",
    "str = string.split('.')\n",
    "\n",
    "if \"springer\" in str:\n",
    "    print(\"yes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
