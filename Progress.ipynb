{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "from random import randint\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettingUrl(url):\n",
    "\n",
    "    options = Options()\n",
    "    ua = UserAgent()\n",
    "    userAgent = ua.random\n",
    "    # print(userAgent)\n",
    "    options.headless = True  # setting this to true makes the browser work in the background\n",
    "    options.add_argument(\"--proxy-server='direct://'\")\n",
    "    options.add_argument(\"--proxy-bypass-list=*\")\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument('--lang=en_US')\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--ignore-certificate-errors\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(f'user-agent={userAgent}')\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    # DRIVER_PATH = \"/usr/bin/chromedriver\"\n",
    "    driver = webdriver.Chrome(service=Service(\n",
    "        ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    driver.get_screenshot_as_file(\"screenshot1.png\")\n",
    "    time.sleep(randint(10,30))\n",
    "\n",
    "    html = driver.page_source\n",
    "    # this renders the JS code and stores all of this information in static HTMl code.\n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting data from acs nano site\n",
    "html = gettingUrl(\"https://pubs.acs.org/doi/10.1021/acsnano.5b05040\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding out the number of times the research paper number 5 is mentioned in the text.\n",
    "# creating an instance of the BeautifulSoup library\n",
    "soup_new = BeautifulSoup(html, 'lxml')\n",
    "a_tags = soup_new.find('div', class_=\"article_content-left ui-resizable\")\n",
    "# print(a_tags)\n",
    "count = a_tags.find_all('a', class_=\"ref5\")\n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to find the last author names of the references\n",
    "soup_new = BeautifulSoup(html, 'lxml')\n",
    "allLastAuthors =[]\n",
    "\n",
    "relevant = soup_new.find('div', class_=\"article_content-left ui-resizable\")\n",
    "string_Refs = relevant.find('p', class_=\"references-count\")\n",
    "string = string_Refs.text\n",
    "number_Refs = re.findall(r'\\d+', string)\n",
    "\n",
    "print(number_Refs[0])\n",
    "\n",
    "i=0\n",
    "\n",
    "for i in range(1, int(number_Refs[0])+1):\n",
    "\n",
    "    info = relevant.find('li', {\"id\":\"ref\"+str(i)})\n",
    "\n",
    "    if info is None:\n",
    "        allLastAuthors.append(\"No information\")\n",
    "    # print(info)\n",
    "\n",
    "    authors = info.find_all('span',class_='NLM_contrib-group')\n",
    "    # print(authors)\n",
    "    last_author = authors[-1].text\n",
    "    allLastAuthors.append(last_author)\n",
    "    i+=1\n",
    "\n",
    "# print((allLastAuthors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an instance of the BeautifulSoup library\n",
    "soup2 = BeautifulSoup(html, 'lxml')\n",
    "links = soup2.find('div', class_=\"article_content-left ui-resizable\")\n",
    "# creating a list containing the google scholar links for all the reference papers.\n",
    "paperLinksgoogleScholar = links.find_all('a', class_=\"google-scholar\")\n",
    "scholarLinks = []\n",
    "for link in paperLinksgoogleScholar:\n",
    "    # A new list containing the google scholar links for only the first three papers is created.\n",
    "    scholarLinks.append(link['href'])\n",
    "for link in scholarLinks:\n",
    "    index = scholarLinks.index(link)\n",
    "    print(index+1, link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_new = []\n",
    "for link in scholarLinks:\n",
    "    html = gettingUrl(link)\n",
    "    # creating an instance of the BeautifulSoup library\n",
    "    soup3 = BeautifulSoup(html, 'lxml')\n",
    "    content = soup3.find('div', class_=\"gs_a\")\n",
    "\n",
    "    if content == None:\n",
    "        print(\"No Data Found\")\n",
    "    else:\n",
    "        authors = content.find_all('a')\n",
    "\n",
    "        # getting the name of the professor to then set up further relationships.\n",
    "        print(authors[-1].text)\n",
    "        authors_new.append(authors[-1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding out the publication house link of the paper from google scholar.\n",
    "paperLink = soup3.find('h3', class_='gs_rt')\n",
    "print(paperLink.a['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding out the name of the research paper by extracting the text from the google scholar page.\n",
    "\n",
    "options = Options()\n",
    "ua = UserAgent\n",
    "userAgent = ua\n",
    "options.headless = True\n",
    "options.add_argument(\"--proxy-server='direct://'\")\n",
    "options.add_argument(\"--proxy-bypass-list=*\")\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument('--lang=en_US')\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "options.add_argument(\"--ignore-certificate-errors\")\n",
    "options.add_argument(\"--disable-extensions\")\n",
    "options.add_argument(f'user-agent={userAgent}')\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(\n",
    "    ChromeDriverManager().install()), options=options)\n",
    "driver.get(scholarLinks[4])\n",
    "# driver.find_element(By.XPATH, value='//*[@id=\"gs_res_ccl_mid\"]/div/div[2]/div[3]/a[2]').click()\n",
    "driver.find_element(By.LINK_TEXT, \"Cite\").click()\n",
    "time.sleep(randint(10, 30))\n",
    "\n",
    "# To get the name of the 5th paper\n",
    "html = gettingUrl(driver.current_url)\n",
    "# print(driver.current_url)\n",
    "soup4 = BeautifulSoup(html, 'lxml')\n",
    "paperName = soup4.find('div', class_='gs_citr')\n",
    "print(paperName.i.text)  # printing the name of the 5th paper.\n",
    "\n",
    "# quits the whole browser session along with all the associated browser windows, tabs and pop-ups.\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction from \"Nature\" publication house.\n",
    "html = gettingUrl(\"https://www.nature.com/articles/s41417-022-00495-w\")\n",
    "\n",
    "\n",
    "# to get the google scholar links of the references.\n",
    "paperLinksgoogleScholarnature = []\n",
    "# creating an instance of the BeautifulSoup library\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "links = soup.find('ol', class_=\"c-article-references\")\n",
    "tags = links.find_all('a', {'data-track-action': 'google scholar reference'})\n",
    "for tag in tags:\n",
    "    # creating a list containing the google scholar links for all the\n",
    "    paperLinksgoogleScholarnature.append(tag['href'])\n",
    "\n",
    "\n",
    "# to get the doi of the references.\n",
    "natureDOI = []\n",
    "refNumber = len(paperLinksgoogleScholarnature)\n",
    "\n",
    "i=0\n",
    "\n",
    "for i in range(1, int(refNumber)+1):\n",
    "    doiLinks = links.find('a', {'aria-label': 'Article reference '+str(i)})\n",
    "\n",
    "    if doiLinks == None:\n",
    "        natureDOI.append(\"No DOI\")\n",
    "    else:\n",
    "        natureDOI.append(doiLinks['href'])\n",
    "    i+=1\n",
    "\n",
    "\n",
    "# extracting the authors from the main page itself.\n",
    "text = soup.find_all('ol', class_='c-article-references')\n",
    "# print(text)\n",
    "text_main = soup.find_all(\n",
    "    'li', class_='c-article-references__item js-c-reading-companion-references-item')\n",
    "# print(text_main[0])\n",
    "\n",
    "Titles = []\n",
    "allLastAuthors_nature = []\n",
    "journalName = []\n",
    "yearPublication = []\n",
    "\n",
    "for ref in text_main:\n",
    "    string = ref.find('p', class_='c-article-references__text')\n",
    "\n",
    "    togetdeets = string.text.split('.')\n",
    "\n",
    "    # returns authors in the form of a string.\n",
    "    allAuthors = togetdeets[0]\n",
    "    allAuthorsList = allAuthors.split(',')  # converting to a list\n",
    "\n",
    "    Titles.append(togetdeets[1])  # gets the title of the references\n",
    "    \n",
    "    journalName.append(togetdeets[2])# gets the journal name of the references\n",
    "\n",
    "    if len(togetdeets) < 4:\n",
    "        yearPublication.append(' ') #gets the year of publication of the references\n",
    "    else:\n",
    "        year = togetdeets[3].split(';')\n",
    "        yearPublication.append(year[0])\n",
    "\n",
    "    if allAuthorsList[-1] == ' et al':\n",
    "        allLastAuthors_nature.append(allAuthorsList[-2])\n",
    "    else:\n",
    "        allLastAuthors_nature.append(allAuthorsList[-1])\n",
    "\n",
    "print(yearPublication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction from \"Springer\" publication house.\n",
    "html = gettingUrl(\"https://link.springer.com/article/10.1007/s43673-022-00064-1\")\n",
    "paperLinksgoogleScholarspringer = []\n",
    "# creating an instance of the BeautifulSoup library\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "links = soup.find('ol', class_=\"c-article-references\")\n",
    "refNumber = len(links)\n",
    "k = 0\n",
    "for k in range(1, int(refNumber)+1):\n",
    "    tags = links.find('a', {'aria-label': 'Google Scholar reference ' +str(k)})\n",
    "    if tags==None:\n",
    "        paperLinksgoogleScholarspringer.append(\"No Google Scholar Link\")\n",
    "    else:\n",
    "        paperLinksgoogleScholarspringer.append(tags['href'])\n",
    "    k+=1\n",
    "# print(paperLinksgoogleScholarspringer)\n",
    "\n",
    "\n",
    "\n",
    "#to get the doi of the references.\n",
    "springerDOI = []\n",
    "refNumber = len(links)\n",
    "# print(refNumber)\n",
    "\n",
    "i=0\n",
    "\n",
    "for i in range(1, refNumber+1):\n",
    "    doiLinks = links.find('a', {'aria-label': 'Article reference '+str(i)})\n",
    "    # print(doiLinks)\n",
    "\n",
    "    if doiLinks == None:\n",
    "        springerDOI.append(\"No DOI\")\n",
    "    else:\n",
    "        springerDOI.append(doiLinks['href'])\n",
    "    i+=1\n",
    "\n",
    "# print(springerDOI)\n",
    "\n",
    "\n",
    "#extracting the authors from the main page itself.\n",
    "text = soup.find_all('ol', class_='c-article-references')\n",
    "\n",
    "k = 0\n",
    "\n",
    "\n",
    "text_main = soup.find_all('li', class_='c-article-references__item js-c-reading-companion-references-item')\n",
    "\n",
    "Titles = []\n",
    "allLastAuthors_springer = []\n",
    "journalName = []\n",
    "yearPublication = []\n",
    "\n",
    "for ref in text_main:\n",
    "    string = ref.find('p', class_=\"c-article-references__text\")\n",
    "\n",
    "    togetdeets = string.text.split(',')\n",
    "    content = togetdeets[-2].split('.')\n",
    "    Titles.append(content[0]) # gets the title of the references\n",
    "\n",
    "    if len(togetdeets)<4:\n",
    "        allLastAuthors_springer.append(\"No Data Found\")\n",
    "    else:\n",
    "        allLastAuthors_springer.append(togetdeets[-3])\n",
    "\n",
    "    sum = ''\n",
    "    for j in range(1, len(content)-1):\n",
    "        sum += content[j]\n",
    "    journalName.append(sum) # gets the journal name of the references\n",
    "\n",
    "    year = togetdeets[-1].split('(')\n",
    "    # print(year[-1])\n",
    "\n",
    "    if year[-1][0]==\" \":\n",
    "        yearPublication.append(\"No Data Found\") # gets the year of publication of the references\n",
    "    else:\n",
    "        year_new = year[-1].split(')')\n",
    "        yearPublication.append(year_new[0]) #gets the year of publication of the references\n",
    "\n",
    "\n",
    "# print(len(springerDOI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction from \"Elsevier(sciencedirect)\" publication house.\n",
    "# html = gettingUrl(\"https://www.sciencedirect.com/science/article/pii/S2376060522000608\")\n",
    "paperLinksgoogleScholarelsevier = []\n",
    "links = []\n",
    "Titles =[]\n",
    "elsevierDOI = []\n",
    "# creating an instance of the BeautifulSoup library\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "tags = soup.find_all('dd', class_=\"reference\")\n",
    "\n",
    "refNumber = len(tags)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for i in range(1, int(refNumber)+1):\n",
    "    tag = soup.find('dd', {'id': 'sref'+str(i)})\n",
    "    print(tag)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# for i in range(1,int(refNumber)+1):\n",
    "#     tag = soup.find('dd', {'id':'sref'+str(i)})\n",
    "#     print(tag)\n",
    "\n",
    "# for tag in tags:\n",
    "#     link = (tag.find_all('a', class_=\"link\"))\n",
    "#     for l in link:\n",
    "#         links.append(l['href'])\n",
    "    \n",
    "#     doi = tag.find('a', class_=\"link\")\n",
    "#     elsevierDOI.append(\"https://www.sciencedirect.com/\"+doi['href'])\n",
    "    \n",
    "\n",
    "# for link in links:\n",
    "#     if \"scholar\" in link:\n",
    "#         paperLinksgoogleScholarelsevier.append(link)\n",
    "\n",
    "# for link in paperLinksgoogleScholarelsevier:\n",
    "#     index = paperLinksgoogleScholarelsevier.index(link)\n",
    "\n",
    "# for tag in tags:\n",
    "#     Titles.append(((tag.find(\"div\", class_=\"contribution\")).find('strong',class_='title')).text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction from \"MDPI\" publication house.\n",
    "paperLinksgoogleScholarMDPI = []\n",
    "toGetDeets=[]\n",
    "Titles=[]\n",
    "mdpiDOI = []\n",
    "allLastAuthors_mdpi = []\n",
    "journalName = []\n",
    "yearPublication = []\n",
    "# html = gettingUrl(\"https://www.mdpi.com/2226-4310/9/11/704/htm\")\n",
    "# creating an instance of the BeautifulSoup library\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "tags = soup.find('ol', class_=\"html-xx\")\n",
    "\n",
    "refNumber = len(tags)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for i in range(1, int(refNumber)+1):\n",
    "    content = tags.find('li', {'data-content': \"{}\".format(i)+'.'})\n",
    "\n",
    "    forName = content.find('span', class_=\"html-italic\")\n",
    "\n",
    "    if forName == None:\n",
    "        journalName.append(\"No Data Found\")\n",
    "    else:\n",
    "        journalName.append(forName.text)\n",
    "\n",
    "    forYear = content.find('b')\n",
    "    \n",
    "    if forYear == None:\n",
    "        yearPublication.append(\"No Data Found\")\n",
    "    else:\n",
    "        yearPublication.append(forYear.text)\n",
    "\n",
    "    toGetDeets.append(content.text.split('[')[0])\n",
    "\n",
    "    if \"CrossRef\" in content.text:\n",
    "        mdpiDOI.append(content.find('a',class_='cross-ref')['href'])\n",
    "    else:\n",
    "        mdpiDOI.append(\"DOI not found\")\n",
    "\n",
    "    if \"Google Scholar\" in content.text:\n",
    "        paperLinksgoogleScholarMDPI.append(content.find('a',class_='google-scholar')['href'])\n",
    "    else:\n",
    "        paperLinksgoogleScholarMDPI.append(\"Google Scholar Link not found\")\n",
    "\n",
    "for deet in toGetDeets:\n",
    "    detail = deet.split(';')[-1]\n",
    "    allLastAuthors_mdpi.append(detail.split('.')[0])\n",
    "    Titles.append(detail.split('.')[:-2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# links = tags.find_all('a', class_=\"google-scholar\")\n",
    "# for link in links:\n",
    "#     paperLinksgoogleScholarMDPI.append(link['href'])\n",
    "#     index = links.index(link)\n",
    "#     print(index+1, link['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction from \"Science\" publication house.\n",
    "# html = gettingUrl(\"https://www.science.org/doi/10.1126/sciadv.abq2104\")\n",
    "paperLinksgoogleScholarScienceall = []\n",
    "paperLinksgoogleScholarScience = []\n",
    "scienceDOI = []\n",
    "toGetDeets = []\n",
    "allLastAuthors_science = []\n",
    "journalName = []\n",
    "Titles = []\n",
    "yearPublication = []\n",
    "# creating an instance of the BeautifulSoup library\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "tags = soup.find('section', {'id': 'bibliography'})\n",
    "forLabels = tags.find_all(\"div\", class_='label')\n",
    "\n",
    "refNumber = len(forLabels)\n",
    "\n",
    "i=0\n",
    "\n",
    "content = tags.find_all('div', class_= \"citation\")\n",
    "for con in content:\n",
    "    toGetDeets.append(con.find('div', class_='citation-content'))\n",
    "\n",
    "for deet in toGetDeets:\n",
    "    if deet.find('em') == None:\n",
    "        journalName.append(\"No Data Found\")\n",
    "    else:\n",
    "        journalName.append(deet.find('em').text)\n",
    "\n",
    "for deet in toGetDeets:\n",
    "    detail = deet.text.split(',')\n",
    "    if ('(' in detail[-1]) == False:\n",
    "        yearPublication.append(\"No Data Found\")\n",
    "    else:\n",
    "        yearPublication.append(detail[-1].split('(')[-1].split(')')[0])\n",
    "\n",
    "for deet in toGetDeets:\n",
    "    Titles.append(deet.text)\n",
    "\n",
    "for con in content:\n",
    "    link = con.find('a')\n",
    "    if link.text == \"Crossref\":\n",
    "        scienceDOI.append(link['href'])\n",
    "    else:\n",
    "        scienceDOI.append(\"No DOI\")\n",
    "\n",
    "for con in content:\n",
    "    link = con.find_all('a')\n",
    "    if (link[-1].text == \"Google Scholar\"):\n",
    "        paperLinksgoogleScholarScience.append(link[-1]['href'])\n",
    "    else:\n",
    "        paperLinksgoogleScholarScience.append(\"Google Scholar Link Not Found\")\n",
    "\n",
    "for i in range(0,refNumber):\n",
    "    allLastAuthors_science.append(\"Next Column for Authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# extraction from \"IEEE\" publication house.\n",
    "# html = gettingUrl(\"https://ieeexplore.ieee.org/document/9837920/references#references\")\n",
    "paperLinksgoogleScholarIEEE = []\n",
    "ieeeDOI = []\n",
    "toGetDeets = []\n",
    "allLastAuthors_ieee = []\n",
    "journalName = []\n",
    "Titles = []\n",
    "yearPublication = []\n",
    "# creating an instance of the BeautifulSoup library\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "tags = soup.find_all('div', class_='reference-container')\n",
    "for tag in tags:\n",
    "    \n",
    "    detail = tag.find('div',class_=\"col u-px-1\")\n",
    "    fineDetail = detail.find('div')\n",
    "    \n",
    "    if fineDetail.find('em') == None:\n",
    "        journalName.append(\"No Data Found\")\n",
    "    else:\n",
    "        journalName.append(fineDetail.find('em').text)\n",
    "\n",
    "    Titles.append(fineDetail.text.split('\"')[-1])\n",
    "\n",
    "    if 'and' in fineDetail.text.split('\"')[0]:\n",
    "        allLastAuthors_ieee.append(fineDetail.text.split('\"')[0].split('and')[-1])\n",
    "    else:\n",
    "        allLastAuthors_ieee.append(fineDetail.text.split('\"')[0])\n",
    "\n",
    "    yearPublication.append(fineDetail.text.split('\"')[-1].split('.')[-2])\n",
    "\n",
    "    doi = tag.find('a', class_=\"stats-reference-link-crossRef\")\n",
    "    \n",
    "    doi_ieee = tag.find('a', class_=\"stats-reference-link-viewArticle\")\n",
    "\n",
    "    if doi == None and doi_ieee == None:\n",
    "        ieeeDOI.append(\"No DOI\")\n",
    "    elif doi == None:\n",
    "        ieeeDOI.append(\"https://ieeexplore.ieee.org\"+doi_ieee['href'])\n",
    "    else:\n",
    "        ieeeDOI.append(doi['href'])\n",
    "\n",
    "    gscholar = tag.find('a', class_=\"stats-reference-link-googleScholar\")\n",
    "    if gscholar == None:\n",
    "        paperLinksgoogleScholarIEEE.append(\"Google Scholar Link Not Found\")\n",
    "    else:\n",
    "        paperLinksgoogleScholarIEEE.append(gscholar['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extraction from \"Cambridge publishing house\" publication house.\n",
    "# html = gettingUrl(\"https://www.cambridge.org/core/journals/advances-in-archaeological-practice/article/professionalcollector-collaboration/8DB3D024A682DEC74457D6D5708B8D73\")\n",
    "paperlinksgoogleScholarCambridge = []\n",
    "content_list = []\n",
    "allLastAuthors_cambridge = []\n",
    "Titles = []\n",
    "yearPublication = []\n",
    "journalName = []\n",
    "cambridgeDOI = []\n",
    "allAuthors = []\n",
    "\n",
    "# creating an instance of the BeautifulSoup library\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "tags = soup.find('div', {'id': 'references-list'})\n",
    "for tag in tags:\n",
    "    content_list.append(tag)\n",
    "\n",
    "content = content_list[3:]\n",
    "refNumber = len(content)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for i in range(2, refNumber+2):\n",
    "    toGetDeets = content[i-2].find('div',\n",
    "                                   {'id': 'reference-'+\"{}\".format(i)+'-content'})\n",
    "\n",
    "    link = toGetDeets.find('a', class_='ref-link')\n",
    "    links = toGetDeets.find_all('a', class_='ref-link')\n",
    "    if link.text == \"CrossRef\":\n",
    "        cambridgeDOI.append(link['href'])\n",
    "    else:\n",
    "        cambridgeDOI.append(\"No DOI\")\n",
    "\n",
    "    if links[-1].text == \"Google Scholar\":\n",
    "        paperlinksgoogleScholarCambridge.append(links[-1]['href'])\n",
    "    else:\n",
    "        paperlinksgoogleScholarCambridge.append(\n",
    "            \"Google Scholar Link Not Found\")\n",
    "\n",
    "        yearPublication.append(toGetDeets.find('span', class_='year').text)\n",
    "\n",
    "    if toGetDeets.find('span', class_='article-title') != None:\n",
    "        Titles.append(toGetDeets.find('span', class_='article-title').text)\n",
    "    else:\n",
    "        Titles.append(\"No data found\")\n",
    "\n",
    "    if toGetDeets.find('span', class_='publisher-name') != None:\n",
    "        journalName.append(toGetDeets.find(\n",
    "            'span', class_='publisher-name').text)\n",
    "    elif toGetDeets.find('span', class_='source') != None:\n",
    "        journalName.append(toGetDeets.find('span', class_='source').text)\n",
    "    else:\n",
    "        journalName.append(\"No Data Found\")\n",
    "\n",
    "    # print(toGetDeets)\n",
    "    author = toGetDeets.find_all('span', class_='string-name')\n",
    "    allAuthors.append(author)\n",
    "\n",
    "for auth in allAuthors:\n",
    "    if auth == []:\n",
    "        allLastAuthors_cambridge.append(\"No Data Found\")\n",
    "    elif len(auth) == 1:\n",
    "        allLastAuthors_cambridge.append(auth[0].text)\n",
    "    else:\n",
    "        allLastAuthors_cambridge.append(auth[-1].text)\n",
    "# Titles\n",
    "# (paperlinksgoogleScholarCambridge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Publication(link):\n",
    "    x = re.findall('\\.(.*?)\\.', link)\n",
    "    return x\n",
    "\n",
    "print(Publication(\"https://link.springer.com/article/10.1007/s43673-022-00064-1\"))\n",
    "print(Publication(\"https://www.nature.com/articles/s41417-022-00495-w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"https://link.springer.com/article/10.1007/s43673-022-00064-1\"\n",
    "string = \"https://www.science.org/doi/10.1126/science.1114655\"\n",
    "string = \"https://www.sciencedirect.com/science/article/pii/S2376060522000608\"\n",
    "string_main = string.split('.')\n",
    "\n",
    "if \"sciencedirect\" in string_main:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for i in range(1,5):\n",
    "    print(\"{}\".format(i)+'.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
